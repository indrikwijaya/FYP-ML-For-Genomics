% Chapter 6

\chapter{Conclusion}
\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{1pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{1pt}
\section{Summary}
\subsection{STEM To Determine Number of Clusters}
We pick STEM as the algorithm that provides the optimum clustering due to a few reasons:
\begin{itemize}
	\item Only STEM that can give us optimal number of clusters from the statistical test
	\item STEM takes into account the sequential nature of time-series data
	\item STEM generates profiles that are independent of the data
	\item Profiles generated by STEM tells us the dynamic between successive time points
\end{itemize}
Nonetheless, since STEM also computes the statistical significance of each cluster, it can exclude many genes which could have been clustered. This omits the possibility of having biologically significant clusters yet not statistically significant clusters as shown in the GO analysis for the other algorithms. Hence, our proposal is that to perform STEM and then extract the optimal number of clusters ($k$) from it. Thereafter, we use $k$ as the initial input for other clustering algorithms and perform GO analysis on the resulting clusters. On the other hand, if the removal of many genes corresponds to removal of noise, STEM may then be used to filter out noise.

\subsection{No Algorithm Performs Better For All Data}
Our results show that no algorithm performs better for all situations. From all the clusters generated by each algorithm, there are always a mix between biologically and statistically meaningful clusters and those that are not. It is also difficult to conclude on what kind of situation or data where one algorithm can perform better than rest. We may need to further preprocess the data to see whether it can improve the performance of each algorithm. As expected, Hierarchical Clustering with Complete and Average linkage performs better than the other algorithms.

\section{Further Work}
\subsection{Different Algorithms}
There are still plenty of clustering algorithms out there that can be used to cluster short time-series data. One possible future plan is to improve the algorithms that we have used previously, such as by incorporating additional data or prior; adjusting the parameters. Here we mention briefly some algorithms that have been improved or modified to cluster short time-series gene expression data:
\subsubsection{Explore On Soft Clustering}
All the algorithms above are considered hard clustering, which means one gene cannot belong to more than one cluster. In reality, one gene can belong to multiple biological groups and thus, may belong to more than one cluster. For this aspect, we can perform Fuzzy-clustering algorithm, a popular soft clustering algorithm. Fuzzy-clustering, modified for short time-series \cite{Moller-Levet2003} has also been used to study the dynamic of gene expression values with short time points. 
\subsubsection{Bayesian Approach}
We can also consider to study variational Bayesian estimation of Gaussian Mixture model \cite{Jia2008} that may improve the performance of GMM. We implement the technique of variational inference, which is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. Due to its Bayesian nature, this model allows us to specify a concentration parameter: weight concentration prior, which helps affect the number of components in the model. Furthermore, high concentration parameter not only leads to more active components in the mixture, but also more uniform weights, which may be useful.
\subsection{Interaction Between Clusters}
As mentioned in Section \ref{5.2}, there may be interaction between clusters which will give more insights on our clustering. One method that has been used is an improvement of HAC, \textit{Fast Optimal Leaf Ordering For Hierarchical Clustering} \cite{fasthac}. Optimal leaf ordering causes input elements that are highly correlated in a cluster to appear in the middle of the linear ordering of the cluster, while marginally related elements are on the borders of the cluster. This improvement reorders the leaves in HAC which gives a more understandable visualizations and hence, helps reveal more biological structures. These relationships are very important in time series data analysis. 
\subsection{Other Distance Measures}
There are also plenty of distance or similarity measures that can be used when we want to cluster time-series data such dynamic time warping distance, short time-series distance developed for Fuzzy-clustering \cite{Moller-Levet2003} and many other. 
